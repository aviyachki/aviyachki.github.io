[
{
	"uri": "https://www.odef.wiki/odef/",
	"title": "The Framework",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Open Detection Engineering Framework A detection engineering story\n"
},
{
	"uri": "https://www.odef.wiki/demm/",
	"title": "DEMM",
	"tags": [],
	"description": "",
	"content": "Chapter 2 Detection Engineering Maturity Model Working with the Detection Engineering Maturity Model\n"
},
{
	"uri": "https://www.odef.wiki/odef/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Purpose ODEF enables organization to create and apply principles and best practices for detection engineering. Utilizing it leads to service and process improvements and maturity.\nThe Framework focuses on using business goals and outcomes to drive and guide cybersecurity activities to deliver detections, improve visibility, minimize vendor dependencies and ultimately improve the organization security posture. The Framework provides the principles that guide efficient and effective detection engineering practices and also provides three maturity levels to measure the organization performance.\nEach phase of the framework’s core aims to describe the detection lifecycle and uses phase functions to focus the effort of the detection engineer and guide them through the process. The three maturity levels provide a high level mechanism for organizations to view and evaluate their approach to detection engineering and focus on areas of improvement.\nHigh level goals The framework high level goals are to:\nProvide guidance on how to be systematic, repeatable and predictable when building hunts and detections Ensure that high visibility is achieved throughout the organization Convert insights to retainable and actionable Knowledge and promote knowledge sharing Introduce continuous vigilance Introduce detection validation through testing Facilitate a knowledge driven environment Framework Mindmap "
},
{
	"uri": "https://www.odef.wiki/demm/intro/",
	"title": "Maturity Model",
	"tags": [],
	"description": "",
	"content": "Introduction Maturity is a self-evaluation process conducted by the team. ODEF provides guidance and structure and assures that all the relevant areas are covered. The goal of the review process is to give a baseline that helps achieving a common understanding about the organization security posture.\nDimentions Threat Detection Content Assurance Knowledge sharing flowchart RL Assurance(Assurance) \u003c---\u003eThreat[Threat Detection Content] Knowledge[Knowledge sharing] \u003c---\u003e Assurance(Assurance) Knowledge[Knowledge sharing] \u003c---\u003e Threat(Threat Detection Content) Maturity levels Level 1 - Partial Threat Detection Content Organizational threat identification practices rely solely on external vendors to provide security content. Assurance and context around alerts and detections is not provided or sufficient. Risk is managed in an ad hoc and often reactive manner by relying on third parties. Assurance There is some limited awareness of cybersecurity threat detection capabilities at the organizational level. The organization implements threat validation and verification on an irregular, case-by-case basis due to varied experience or information gained from outside sources. Assurance through continuous validation is not present. Knowledge sharing The organization may not have processes to enable cybersecurity information sharing. Documentation is rarely written and shared only on ad-hoc basis and it is scattered across teams. Level 2 - Adequate Threat Detection Content Organizational threat identification practices rely on internal teams and external vendors to provide security content. Context around alerts and detections is provided. Specialized teams are able to introduce new detections and security content. Some security teams have a better understanding of security posture than others. Assurance There is some awareness of cybersecurity threat detection capabilities as the organization is now building custom detections to compensate for gaps. The custom detections are use case driven and validated during the detection development process. Continuous validation is not enabled and the organization still relies on suppliers for most of the detection capabilities. Knowledge sharing The organization is starting to enable knowledge sharing and promotes documentation efforts. There is a central detection information repository. Level 3 - Enabled (Proactive) Threat Detection Content Organization maintains continuous practices that provide excellent internal insights and knowledge. Context around alerts and detections is provided. Any team is encouraged and capable to introduce new detection components and thus improve the security posture. The security posture of the environment is well understood across the security teams. Assurance The organization possesses a detection coverage map and covers a big percentage with in-house built detections. The organization does not rely on vendors to provide security content. Automation is provided to continuously validate and run the detection use cases. Additional assurance is achieved by running red team exercises and automation frameworks. Knowledge sharing Organizations possess practices to create and maintain high quality records and appropriately control and manage the access to the information. Processes for socializing detections are automated and teams are informed of the development of new detections. Operational Maturity Maturity Review Process (MRP) The process of evaluating the maturity:\nCollect - Collect information about your processes,people and tools. Identify changes to any. The goal is to gain a holistic understanding of the organization\u0026rsquo;s security teams, tools and processes. Based on that information various posture improvements can be identified.\nAnalize - Based on the data that you have collected and find the corresponding maturity level. Finding where in the maturity level the organization is important for understanding the impact and importance of each identified security improvement initiative and thus prioritize accordingly.\nPrioritize - Prioritize and decide which is the next low hanging fruit that can be improved. Not all security issues are equally important, prioritization should focus on those initiatives that influence and change the security posture and introduce the most maturity.\nImprove - Create an initiative or a project for improving the identified gap.\nSecurity Improvement Initiative Security improvement initiatives are likely outcomes of the MRP process. The goal of the security improvement initiative is to address identified visibility gaps in the organization\u0026rsquo;s security posture. For example, during the review process or detection engineering we may identify that our application is not providing sufficient logging in order to detect particular behavior or ttp of interest. That is a good candidate for a security improvement initiative. The goal of the initiative would be to deliver the visibility needed and notify back the Detection Engineer so that they can proceed with the detection creation. Depending on the size of the organization and internal processes, this process might be driven by the Detection Engineer or completely separate team.\nDEMM Cadence Evaluating the maturity of the organization and striving to improve it is no one time effort or activity. For that best results can be achieved by:\nSet a regular schedule for reevaluating and revisiting the DEMM. Ensure that the security improvement initiatives are targeted with a timeline and aligned with the overall organizational security strategy.\n"
},
{
	"uri": "https://www.odef.wiki/demm/operational-maturity/",
	"title": "Operational Maturity",
	"tags": [],
	"description": "",
	"content": "Maturity Review Process (MRP) The process of evaluating the maturity:\nCollect - Collect information about your processes,people and tools. Identify changes to any. The goal is to gain a holistic understanding of the organization's security teams, tools and processes. Based on that information various posture improvements can be identified. Analize - Based on the data that you have collected and find the corresponding maturity level. Finding where in the maturity level the organization is important for understanding the impact and importance of each identified security improvement initiative and thus prioritize accordingly. Prioritize - Prioritize and decide which is the next low hanging fruit that can be improved. Not all security issues are equally important, prioritization should focus on those initiatives that influence and change the security posture and introduce the most maturity. Improve - Create an initiative or a project for improving the identified gap. Security Improvement Initiative Security improvement initiatives are likely outcomes of the MRP process. The goal of the security improvement initiative is to address identified visibility gaps in the organization's security posture. For example, during the review process or detection engineering we may identify that our application is not providing sufficient logging in order to detect particular behavior or ttp of interest. That is a good candidate for a security improvement initiative. The goal of the initiative would be to deliver the visibility needed and notify back the Detection Engineer so that they can proceed with the detection creation. Depending on the size of the organization and internal processes, this process might be driven by the Detection Engineer or completely separate team. DEMM Cadence Evaluating the maturity of the organization and striving to improve it is no one time effort or activity. For that best results can be achieved by: Set a regular schedule for reevaluating and revisiting the DEMM. Ensure that the security improvement initiatives are targeted with a timeline and aligned with the overall organizational security strategy. "
},
{
	"uri": "https://www.odef.wiki/knowledge/doc-template/",
	"title": "Documentation Template",
	"tags": [],
	"description": "",
	"content": "Template should be used to standardize the detection content and create a knowledge base\nDetection Summary Status (developing/active/decommissioned) Goal (Why) The goal of the detection TTP (What) Mitre attack link Strategy (How) Query or short explanation about it Automation (When) Schedule and automation details Sources (Where) Data sources used for the detection Severity high/medium/low Priority high/medium/low Research Goal The goal section provides the intended purpose of the alert. It is a simple, plaintext description of the type of behavior you\u0026rsquo;re attempting to detect and why.\nCategorization The categorization is a mapping of the detection to the relevant entry in the MITRE ATT\u0026amp;CK. This is used in reporting with tools such as Mitre Att\u0026amp;CK Navigator to visualize coverage of TTPs and provide assurance. Additionally when a TTP is mapped to MITRE it can be used to perform attacker cyber attributed.\nTechnical Context Technical Context provides detailed information and background needed for a responder or an engineer to understand all components of the detection.The goal of the section is to include technical research for the TTP and additionally how it relates to the environment. It can help incident responders to understand better the alert and also security engineers in order to address a technical security gap.\nDetection Summary Is a high-level walkthrough of how the detection/hunt works. This describes what the alert is looking for, what technical data sources are used, any enrichment that occurs, and any false positive minimization steps.\nSeverity Severity is a measurement of impact. How much impact does an incident have on the overall security of the business? Some TTPs are clear indicator for attacker present in the environment, those will have higher severity than others. For example detection for dcsynch vs detection for received phishing email(without any confirmation for clicked link).\nPriority Priority should be based on the detection severity. The goal of prioritization is to allow your SOC analyst and Incident Responders to focus on the most pressing issues first.\nPrepare Dataset Identify the appropriate source of information which will be used in the detection and document it here.\nVisibility Check Ensure there is sufficient logging, retention and visibility. Provide evidence (screenshots, json files etc) that prove that there is sufficient visibility and logging to collect and build detection logic.\nBuild \u0026 Enrich Detection Creation Create a detection query against the identified dataset. Document the queries used here and provide details of the logic.\nManual Testing Perform manual testing against production data and ensure minimal False Positives. Document your test searches.\nBaseline development Based on the results from your manual testing you may or may not need to develop a baseline. Baseline is a set of normal behaviours which are excluded from the detection to minimize noise and increase fidelity\nBlind Spots and Assumptions Think about issues which could prevent your detection from alerting. For example, lack of visibility due to missing endpoint agent, or particular string which, in case is modified, the detection will not work etc.\nUnittest Development Create automated unittest that will cover:\nChanges or missing data Syntax errors To confirm detection logic by performing true positive detection Enrich Utilize or develop enrichment capability to support the detection if needed. There are external and internal sources of enrichment. In your pipeline or SIEM you should be able to interact with these sources and collect data as needed. For example, consider user behavioral analytic use case which requires to know when a user is on vacation - this would require access to an up-to-date HR database.\nValidate Validation are the steps required to generate a representative true positive event which triggers this alert. This can be a walkthrough of steps used to generate an alert, a script to trigger the detection (such as Red Canary\u0026rsquo;s Atomic Red Team Tests), or a scenario used in an alert testing and orchestration platform.\nEach alert / detection strategy must have true positive validation. This is a testing process designed to prove the true positives are detected.\nTrue positive validation To perform positive validation:\nGenerate a scenario where a true positive would be generated. Document the process of your testing scenario. From a testing device, generate a true positive alert. Validate the true positive alert was detected by the strategy. False positive validation FP validation is yet another confirmation that when ran in production the detection is not producing excessive number of alerts from standard events in the organization - for example software compilation etc.\nAutomate The idea of the section is to provide information on how the query is automated and what is the schedule of execution. Document any interaction between the different environments or applications that may be involved. For example, if your detection uses api calls to enrich from an HR database and then utilizes a scoring model from a micro service are interactions that should be documented here.\nShare Socialize the detection Follow the process for socializing the detection with the receiving team/s (Fraud, SOC\u0026amp;IR, Engineering, Hunting etc). The receiving team should acknowledge and accept the new detection after performing a quality check.\nResponse The SOC and Incident Response teams should align the response to any alerts from the detection to their standard response playbooks - for malware, insiders etc. In case of absence of IR playbooks - the response plan can be documented here.\nAppendix Include any external links and references. "
},
{
	"uri": "https://www.odef.wiki/odef/framework-core/",
	"title": "Framework Core",
	"tags": [],
	"description": "",
	"content": "Framework Core The Framework Core provides a set of activities to achieve specific cybersecurity outcomes and references examples of how to achieve those outcomes.\nFunctions, Goals, Guidelines There are set of functions, goals and guidelines for each phase from the detection lifecycle. Functions, goals, guidelines help the detection engineer to have north star focus and deliver a detection with exceptional quality.\nFunctions Similarly to python functions those are single goal activities that return or drive particular outcome. {{Example}}\nGoals Each function aims to deliver defined and desired result. Although goals are high level and could be abstract it recommended to a clearly defined one-to-one relationship between functions and goals.\nGuideline Guidelines are flexible documents that support achieving set goal in a particular function. Example: Each company can have their own Change Management processes and any document describing the process can be considered guideline.\nPhases The Core comprises of three lifecycle phases:\nSunrise Midday Sunset These phases describe the complete life of a detection - from inception to decommissioning.\n"
},
{
	"uri": "https://www.odef.wiki/odef/phases/sunrise/",
	"title": "Sunrise",
	"tags": [],
	"description": "",
	"content": "Phase 1️⃣ Sunrise 🌅 Sunrise is the first phase of the detection lifecycle. It marks the inception, development and deployment of the detection. During that phase there are 6 core functions that should be addressed:\nResearch Prepare (Logging) Build (Detection Content) Validate Automate Share (Knowledge) High level goals for the Sunrise phase Build high fidelity detection Ensure detection validation Create documentation Integrate and automate in the environment Socialize the detection with the security organization Functions Goal Description Guidelines Research Opportunity Identification It can be triggered from analyzing threat intelligence reports, or OSINT, or internal knowledge for a particular security gap. Document the use case and the goals of the detection as part of the opportunity identification process. Document the use case that you’re building and set goals. Is the TTP already covered by an existing alert or detection? Is there sufficient knowledge to start building or additional research would be required? What are sources of information that will assist the research? Prioritize Detection engineering work has to be prioritized and tracked. Work prioritization can be based on urgency and priority. Backlog of detections and security posture activities is desirable and recommended. Prioritization criteria: Criticality of the system\nHighest level of threat to the organization\nEase of Exploitation\nPast incidents\nDevelop Research Questions Write your research questions that while answering you will gain understanding of the topic. Examples: Write down what you already know or don\u0026rsquo;t know about the topic. Use that information to develop questions. Use probing questions. (why? what if?). Avoid \u0026ldquo;yes\u0026rdquo; and \u0026ldquo;no\u0026rdquo; questions Information Gathering Research and collect sufficient information in order to start understanding the detection Provides a good overview of the topic if you are unfamiliar with it. Identify important facts, dates, events, history, organizations, etc. (in case the detection is a response to a past incident.) Find bibliographies which provide additional sources of information (include in the Appendix section detection document) Technical Context Create and understand technical context around the detection Start putting technical writeup by summarizing the most important information from technical aspect Research the technology associated with the technique to help understand the use cases, related data sources, and detection opportunities Note: Defenders often create superficial detections because they lack an understanding of the technology involved. In case of uncertainties it is best to engage the team or engineer responsible for the management of the technology Prepare\nIdentify Dataset Identify the log source that will be used for the detection Know your environment Understand the data source and document it by creating a data dictionary. The data dictionary should grow and contain sources of data and their corresponding schemas. It can later be used to quickly refer to. Visibility Check Ensure there is sufficient logging, retention and visibility in order to successfully build the detection and satisfy the use case Use the accumulated technical knowledge to identify source and identify the events required to build detection Use any historical events in order to validate that there is sufficient visibility Improve(optional) Once the data is explored we can identify opportunities for improvements such as: Collecting additional logs or change logging levels Create additional attributes (parsing of raw logs) Consolidation of distinct logs Improvement initiatives and requests should be communicated to the responsible for the dataset in question team. For that purpose it makes sense to maintain a contact list that provides quick reference to technology, support/engineering teams and contact details. Build \u0026amp; Enrich Detection Creation Create a detection query against the identified dataset Having a good understanding of the technical context and the data source begin building queries to narrow down the data to actionable insight. Manual Testing Perform a manual testing and ensure the query works syntax and logical perspective Ensure the query does not have any syntax errors In case the detection is build in response to past incident ensure that the query is indeed catching true positive events Baseline development Develop a baseline (if needed) that will improve the detection fidelity Baselines are sets of known and verified good behaviors and events present in the organization. Those events are normally excluded from the detection logic. Baselines decisions and considerations should be documented and clearly states in the ADS Baselines are included in the hunt.yml/tf/hcl or alert.yml/tf/hcl files Unittest Development The unittest development is dependent on the type of devops pipeline. Simple goals are provided. Goals for the unittesting: Changes or missing data Syntax errors To confirm detection logic by performing true positive detection Enrich Enrich with additional data source if required Each hunt could have different enrichment requirements. In some cases HR database could be used in order to understand if a person is on vacation, other trivial cases could be lookup of a hash, ip or domain in an threat intelligence repository etc. Document Create KB Document Complete the ADS Mitre minefield update Central knowledge base repository is required in order to mature the detection engineering program. This can be a github repository with controlled access that provides on a need to know basis the security teams members with access. Each hunt should have a corresponding README.MD file that provides sufficient information and context. Consider an SOC analyst or Incident Responder responding to an event from your detection. By looking at the documentation they should be easily briefed on the premise and technicalities of the detection. Validate Confirm unittests Confirm unittest are working Confirmation of the unittests can be done by inspecting the implemented devops pipeline and ensuring that the actions (in the case of github) for unittests are running True Positive validation Validate true positive event against real dataset using the query developed earlier. True positive validation can be achieved by: Using historical event that exists in the central data repository Emulation of the TTP by executing it in a controlled environment False Positive Validation Ensure no FP are produced by the query when ran against the prod dataset. False positive events are good known events which are produced as output results by the detection/hunt query. If baseline is used it should be validated that the baseline is catching those good known events. Splunk example: Splunk you can use makeresult command to create fake results and test your baseline and how you handle false positives. Automate Automation \u0026amp; deployment This step is entirely dependant on the environment and should follow the standard ci/cd or automation practices of the organization. Integrate with devops pipeline and enable continuous deployment Share Socialize the new detection A notification process is required and it should be created. The process can be in the form of newsletter or slack channel notification, preferably automated one. Follow a process to communicate the newly created detection with the Security Teams and inform them about it Update Sec Dependency Tree This document is actually part of the repository and can be shared with data engineering and security teams. The goal of sharing it is to promote care mentality where teams would check before they change. Meaning, if data engineer is about to rename an index they should first check if the index is being used. Having dependency document as part of the repository makes it easy and seamless for them to check. Update organization wide document showing dependencies for the detections Process Flow graph TD; Research1(Opportunity Identification) --\u003eResearch2(Prioritize); Research2 --\u003eResearch3(Develop Research Questions); Research3 --\u003eResearch4(Information Gathering); Research4 --\u003eResearch5(Collect Technical Context); Research5 --\u003ePrepare1(Identify Dataset); Prepare1 --\u003ePrepare2(Visibility Check); Prepare2 --\u003ePrepare3{Improve}; Prepare3 --\u003e |yes| cis[Start security improvement initiative]; Prepare3 --\u003e |no| Build1(Detection Query Creation); Build1 --\u003e Build2(Manual Testing); Build2 --\u003e Build3(Baseline development); Build3 --\u003e Build4(Automated Unittest Development); Build4 --\u003eBuild5(Enrich); Build5 --\u003e Build6(Document); Build6 --\u003e Validate1(Confirm unittests); Validate1 --\u003eval2(True/False Positive validation); val2--\u003eautomate(Automation \u0026 deployment); automate --\u003e share(Socialize the new detection); share --\u003eshare1(Update Sec Dependency Tree); "
},
{
	"uri": "https://www.odef.wiki/odef/phases/midday/",
	"title": "Midday",
	"tags": [],
	"description": "",
	"content": "Phase 2️⃣ Midday ☀️ The “Midday” phase is normally the longest phase from the detection lifecycle, during which the detection has been engineered and commissioned to production. The phase monitors the detection during its operation and aims to improve it if needed. High level goals for the Midday phase: Operate and monitor the detection for FP or TP Improve the detection logic in case of influx of FP Perform systematic reviews to ensure relevancy Functions Goal Description Guidelines Monitor\u003c/\u003e Run as per defined schedule Detection is configured to run on pre-defined schedule or real time if applicable Detections will run based on the schedule set during the sunrise phase. Confirm unittest passing Monitoring is configured to notify the responsible team in case the automation for the detection is not running properly Suggested approach: github actions - before deployment ensuring proper syntax Work detections Once detection is running it should be monitored for any TP or potential influx of FP TP events should be triaged, investigated and responded on by following an agreed IR process. FP events should be investigated, proved as FP and documented as part of the baseline. Once the baseline is changed in the documentation the query can be updated and improved. Measure Measure detection efficacy Enable metrics for the detection based on which areas for improvement can be identified. Mitre Attack weakness\nSuccess/failure of automating detections\nServices covered Each detection that covers particular TTP can be marked in the Mitre ATT\u0026amp;CK Navigator. Looking at percentage of covered tactics and techniques can be a metric. Success or Failure in detection automation or influx of FP metric can be used to identify detections that require improvement. Detection runtime length is a metric which can identify poorly written queries. For example, query too open that collects way too many events and chunks too much data only to spend even more time to filter by using custom logic. Improve (optional) Improve detection fidelity Once improvement opportunities have been identified during the operations or periodic review an improvement is triggered The goal of this function is to improve any detections which are with poor health (slow runtime, causing errors) and improve them by revisiting the detention logic. Review Perform periodic review Review detections to identify improvement opportunities or decommission requirements Detection can become irrelevant and thus decommissioned whe: The risk that it is compensating is far smaller than the cost of running the detection\nThe technology used for the detection is no longer present in the company Midday phase Process Flow graph TD; Monitor1(Run per schedule) --\u003eMonitor2(Receive alerts); Monitor2(Respond to alerts) --\u003e Monitor3{False Positives?} ; Monitor3 --\u003e |no| Measure[Document TP]; Measure --\u003e Review(Perform periodic review) Monitor3 --\u003e |yes| Improve(Improve); Improve --\u003e Monitor1; "
},
{
	"uri": "https://www.odef.wiki/odef/phases/sunset/",
	"title": "Sunset",
	"tags": [],
	"description": "",
	"content": "Phase 3️⃣ Sunset 🌆 During the “Sunset” phase the detection is taken out of commission. The phase wants to ensure that resources are not spent for outdated detections that are no longer applicable and at the same time leave sufficient trace of the existence of the detection. High level goals for the Sunset phase:\nDecommission the detection and leave it in a state that it can be resumed anytime Preserve knowledge Functions Goal Description Guidelines Decommission Decommission the detection The goal is to decommission the detection by following process that provides visibility In order to decommission a detection simply change the status field to \"Sunset\" in the .yml file. Assuming your devops pipeline is configured correctly, this should effectively disable the detections and prevent it from running. Note: Do not remove anything from the repository as detections can be reused in future. Knowledge base update Create an adequate indication in the KB document that the detection is no longer active and socialize the change with your security teams. Update Mitre coverage map by removing the coverage that the detection was providing Sunset Process Flow graph TD; Review1[Review completed] --\u003e Review2; Review2{detection ready to decom} --\u003e|no| End[end]; Review2{detection ready to decom} --\u003e|yes| Preserve(Preserve knowledge); Preserve --\u003e Decommission(Decommission the detection); "
},
{
	"uri": "https://www.odef.wiki/dac/",
	"title": "DAC",
	"tags": [],
	"description": "",
	"content": "Chapter 3 Detection as Code Working with the Detection as Code\n"
},
{
	"uri": "https://www.odef.wiki/dac/odef-templates/",
	"title": "Templates",
	"tags": [],
	"description": "",
	"content": "Template Purpose ODEF provides two templates for documenting detections — yaml and markdown. Each for different purpose:\nYaml is used due to its data serialization and wide programming language compatibility. It is used for automation and integrations with other systems. It stores components like the queries, baseline, schedule and others. It is a stepping stone for Detection-as-Code capability.\nMarkdown is used for detection documentation due to its readability and simplicity. Especially helpful for knowledge sharing when used in conjunction with platforms like GitHub. The purpose of the file is to house all details related to the detection. Check the Knowledge Management section for additional information.\n"
},
{
	"uri": "https://www.odef.wiki/dac/yml-file/",
	"title": "The yaml file",
	"tags": [],
	"description": "",
	"content": "Yaml file purpose status: \u0026#34;{{ status }}\u0026#34; created_date: \u0026#34;{{ created_date }}\u0026#34; last_updated_date: name: \u0026#34;{{ detection_name }}\u0026#34; query: \u0026#34;{{ query }}\u0026#34; author: \u0026#34;{{ detection_author }}\u0026#34; schedule: \u0026#34;{{ schedule }}\u0026#34; baseline: \u0026#34;{{ baseline }}\u0026#34; visualization: \u0026#34;{{ visualization }}\u0026#34; event_limit: 0 data_source: \u0026#34;{{ data_source }}\u0026#34; data_location: \u0026#34;{{ data_source }}\u0026#34; tactic: \u0026#34;{{ tactic }}\u0026#34; mitre_id: \u0026#34;{{ mitre_id }}\u0026#34; mitre_url: \u0026#34;{{ mitre_url }}\u0026#34; incident: severity: \u0026#34;numeric, 0-unknown, 0.5 - informational, 1-low,2-medium,3-high,4-critical\u0026#34; type: \u0026#39;Security Incident\u0026#39; name: \u0026#39;string, name of the incident\u0026#39; description: \u0026#39;inc descr: This detection is monitoring for changes in any of X\u0026#39; sla: \u0026#39;integer - number of minutes added to incident create time(incident sla). example sla: 1440 this means 24h sla\u0026#39; "
},
{
	"uri": "https://www.odef.wiki/knowledge/",
	"title": "Knowledge management",
	"tags": [],
	"description": "",
	"content": "Chapter 4 Knowledge management The importance of knowledge in an organization\n"
},
{
	"uri": "https://www.odef.wiki/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "Welcome ✨ ODEF is a detection engineering framework that helps with building detection engineering capabilities ✨\nResources ODEF is open sourced framework and can be found here\nTable of Content The Framework Phases Sunrise Midday Sunset DEMM DAC Knowledge Management License Licensed under the MIT License. Copyright (c) 2023 Atanas Viyachki.\n"
},
{
	"uri": "https://www.odef.wiki/odef/phases/",
	"title": "Phases",
	"tags": [],
	"description": "",
	"content": "Intro ODEF is the first framework that defines a detection lifecycle. With three phases — sunrise, midday and sunset ODEF covers the life of a detection from inception to decommissioning. Each phase has corresponding functions, goals and guidelines.\n"
},
{
	"uri": "https://www.odef.wiki/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://www.odef.wiki/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]